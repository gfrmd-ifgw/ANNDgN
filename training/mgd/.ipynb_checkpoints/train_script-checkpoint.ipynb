{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the script used to train de MGD NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###########Import modules##############\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']='0'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "#force CPU to guarantee repetable results (issue with GPU seeding)\n",
    "#comment the line to allow GPU training (if your system is configured and supports it)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "seed = 191119 #random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "import keras as ks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, ActivityRegularization, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras import regularizers, initializers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262223, 9)\n"
     ]
    }
   ],
   "source": [
    "############Read the data###########\n",
    "df = pd.read_csv('mgd_data.txt', sep='\\t')\n",
    "\n",
    "\n",
    "#just in case, find if any duplicate exists\n",
    "df.drop_duplicates(subset=['Energy (keV)', 'Breast Radius (cm)', 'Breast Thickness (cm)', 'Breast Glandularity', \n",
    "       'Skin Thickness (mm)', 'Adipose Thickness (mm)'], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#return df shape\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our feature matrix\n",
    "X = df[['Energy (keV)', 'Breast Radius (cm)', 'Breast Thickness (cm)', 'Breast Glandularity', \n",
    "       'Skin Thickness (mm)', 'Adipose Thickness (mm)']].values\n",
    "\n",
    "\n",
    "#Our label vector, notice the min norm\n",
    "Y = (df[['MGD(mGy/hist)', 'Rel. Error']].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we separate train and test samples (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed,shuffle=True)\n",
    "\n",
    "######Apply min max transformation into X train\n",
    "min_max_scaler = preprocessing.MinMaxScaler() #preprocessing.StandardScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "minimum =  np.min(y_train[:,0])\n",
    "\n",
    "np.savetxt('trained_model/dgm_min_norm_value.txt', [minimum])\n",
    "\n",
    "y_train[:,0] = y_train[:,0]/minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start k fold\n",
    "kf = KFold(n_splits=5, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167822 samples, validate on 41956 samples\n",
      "Epoch 1/500\n",
      "167822/167822 [==============================] - 55s 325us/step - loss: 87.7416 - mean_absolute_percentage_error: 83.9082 - val_loss: 76.7534 - val_mean_absolute_percentage_error: 69.1522\n",
      "Epoch 2/500\n",
      " 70250/167822 [===========>..................] - ETA: 28s - loss: 74.5845 - mean_absolute_percentage_error: 67.4286"
     ]
    }
   ],
   "source": [
    "model_vec = []\n",
    "error_vec = []\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "my_init = initializers.glorot_uniform(seed=seed)\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train1, X_test1 = X_train[train_index], X_train[test_index]\n",
    "    y_train1, y_test1 = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    y_gen = np.random.normal(y_train1[:,0], scale = y_train1[:,0]*y_train1[:,1]*0.5)\n",
    "\n",
    "    model = Sequential()\n",
    "    #First layer\n",
    "    model.add(Dense(500,input_dim=6, activation='relu', kernel_regularizer=regularizers.l2(0.000),\n",
    "                    activity_regularizer=regularizers.l2(0.0001), \n",
    "                    kernel_initializer=my_init))\n",
    "    #Second layer\n",
    "    model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.000),\n",
    "                    activity_regularizer=regularizers.l2(0.0001), \n",
    "                    kernel_initializer=my_init))\n",
    "    #Third layer\n",
    "    model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.000),\n",
    "                    activity_regularizer=regularizers.l2(0.0001), \n",
    "                    kernel_initializer=my_init))\n",
    "    #linear output\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    #optimizer\n",
    "    adam = optimizers.Adam(lr=0.001, decay=0.0001)\n",
    "\n",
    "    model.compile(optimizer=adam,\n",
    "              loss='mean_absolute_percentage_error',\n",
    "              metrics=['mape'])\n",
    "    \n",
    "    #train\n",
    "\n",
    "    model.fit(X_train1, y_gen, epochs=500, batch_size=250, verbose=1, \n",
    "              validation_data=(X_test1,y_test1[:,0]), callbacks=[early_stopping])\n",
    "\n",
    "    #append trained model\n",
    "    model_vec.append(model)\n",
    "    \n",
    "    \n",
    "\n",
    "    #calculateds the % of the training error\n",
    "    y = (model.predict(X_test1))\n",
    "    y = np.ravel(y)\n",
    "    error = 100*(y- y_test1[:,0])/y_test1[:,0]\n",
    "    error_vec.append(error)\n",
    "    \n",
    "    plt.boxplot(error_vec)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots training error\n",
    "plt.boxplot(error_vec)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compares with the validation data\n",
    "#First transform X val.\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "\n",
    "#For all trained models, evaluate\n",
    "y_predict_vec = []\n",
    "for model in model_vec:\n",
    "    y_predict = model.predict(X_test_minmax)\n",
    "    y_predict = y_predict*minimum\n",
    "    y_predict_vec.append(y_predict)\n",
    "    \n",
    "    \n",
    "#average between models\n",
    "y_mean = np.mean(y_predict_vec, axis=0)\n",
    "y_mean = np.ravel(y_mean)\n",
    "#std between models\n",
    "y_std = np.std(y_predict_vec, axis=0, ddof=1)\n",
    "y_std = np.ravel(y_std)\n",
    "\n",
    "#difference\n",
    "dif = 100*(y_mean-y_test[:,0])/y_test[:,0]\n",
    "\n",
    "#boxplot of the validation error distribution\n",
    "plt.boxplot([dif])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#output for error distribution\n",
    "np.savetxt('y_mean_dgm.out', y_mean)\n",
    "np.savetxt('y_test_dgm.out', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots error distribution\n",
    "plt.figure(figsize=(3,3), dpi=500)\n",
    "plt.hist(dif, bins=20)\n",
    "plt.xlabel('Relative Difference (%)', size=10)\n",
    "plt.ylabel('Count', size=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust line between test and predict values\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(y_test[:,0],y_mean)\n",
    "print(slope, intercept, r_value**2)\n",
    "\n",
    "plt.plot(y_test[:,0], y_mean, 'o', ms=1)\n",
    "plt.plot(y_test[:,0], y_test[:,0], ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "#saves trained models\n",
    "for idx,model in enumerate(model_vec):\n",
    "    model.save('trained_model/trained_model'+str(idx)+'.h5')\n",
    "    \n",
    "    \n",
    "import pickle\n",
    "#saves scaler\n",
    "with open(\"trained_model/min_max.pkl\", 'wb') as file:\n",
    "    pickle.dump(min_max_scaler, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('error_vec_training_dgm', error_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
